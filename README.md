# LeetLLM

## 1. QAs

## 2. Transformers

### 2.1 Transformer

- https://nlp.seas.harvard.edu/annotated-transformer/

#### 2.1.1 Self-Attention

#### 2.1.2 Cross-Attention

- https://magazine.sebastianraschka.com/p/understanding-multimodal-llms

  ![img](https://substackcdn.com/image/fetch/$s_!3PZD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png)


#### 2.1.3 Multi-Head Attention

#### 2.1.4 Layer Norm

#### 2.1.5 Positional Encoding

## 3. LLMs

- https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison
![img](https://substackcdn.com/image/fetch/$s_!iCn-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png)

### 3.1 DeepSeek

### 3.2 Qwen

### 3.3 Kimi

### 3.4 GPT

### 3.5 Llama

### 3.6 Olmo

- https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb

## 4. Training

### 4.1 Pre-Training

#### 4.1.1 Scaling law

- https://blogs.nvidia.com/blog/ai-scaling-laws/

### 4.2 Post-Training

#### 4.2.1 SFT (Instruction-Tuning, Multi-Task)

#### 4.2.2 RLHF

#### 4.2.3 RLVR

#### 4.2.4 Test-Time Scaling

#### 4.2.5 Test-Time Training

- https://yueatsprograms.github.io/ttt/home.html